{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Admission_Predict_Ver1.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Serial No.', 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n",
       "       'LOR ', 'CGPA', 'Research', 'Chance of Admit '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA   \n",
       "0           1        337          118                  4  4.5   4.5  9.65  \\\n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>144.481833</td>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>125.750000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.500000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>375.250000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Serial No.   GRE Score  TOEFL Score  University Rating         SOP   \n",
       "count  500.000000  500.000000   500.000000         500.000000  500.000000  \\\n",
       "mean   250.500000  316.472000   107.192000           3.114000    3.374000   \n",
       "std    144.481833   11.295148     6.081868           1.143512    0.991004   \n",
       "min      1.000000  290.000000    92.000000           1.000000    1.000000   \n",
       "25%    125.750000  308.000000   103.000000           2.000000    2.500000   \n",
       "50%    250.500000  317.000000   107.000000           3.000000    3.500000   \n",
       "75%    375.250000  325.000000   112.000000           4.000000    4.000000   \n",
       "max    500.000000  340.000000   120.000000           5.000000    5.000000   \n",
       "\n",
       "            LOR         CGPA    Research  Chance of Admit   \n",
       "count  500.00000  500.000000  500.000000         500.00000  \n",
       "mean     3.48400    8.576440    0.560000           0.72174  \n",
       "std      0.92545    0.604813    0.496884           0.14114  \n",
       "min      1.00000    6.800000    0.000000           0.34000  \n",
       "25%      3.00000    8.127500    0.000000           0.63000  \n",
       "50%      3.50000    8.560000    1.000000           0.72000  \n",
       "75%      4.00000    9.040000    1.000000           0.82000  \n",
       "max      5.00000    9.920000    1.000000           0.97000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial No.             int64\n",
       "GRE Score              int64\n",
       "TOEFL Score            int64\n",
       "University Rating      int64\n",
       "SOP                  float64\n",
       "LOR                  float64\n",
       "CGPA                 float64\n",
       "Research               int64\n",
       "Chance of Admit      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Serial No.'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research   \n",
       "0        337          118                  4  4.5   4.5  9.65         1  \\\n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,0:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "495        332          108                  5  4.5   4.0  9.02         1\n",
       "496        337          117                  5  5.0   5.0  9.87         1\n",
       "497        330          120                  5  4.5   5.0  9.56         1\n",
       "498        312          103                  4  4.0   5.0  8.43         0\n",
       "499        327          113                  4  4.5   4.5  9.04         0\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>310</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>300</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>322</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>307</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>326</td>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>300</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "238        310          104                  3  2.0   3.5  8.37         0\n",
       "438        318          110                  1  2.5   3.5  8.54         1\n",
       "475        300          101                  3  3.5   2.5  7.88         0\n",
       "58         300           99                  1  3.0   2.0  6.80         1\n",
       "380        322          104                  3  3.5   4.0  8.84         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "255        307          110                  4  4.0   4.5  8.37         0\n",
       "72         321          111                  5  5.0   5.0  9.45         1\n",
       "396        325          107                  3  3.0   3.5  9.11         1\n",
       "235        326          111                  5  4.5   4.0  9.23         1\n",
       "37         300          105                  1  1.0   2.0  7.80         0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46      ,  0.5       ,  0.25      ,  0.375     ,  0.14285714,\n",
       "         0.5224359 ,  0.        ],\n",
       "       [ 0.44      ,  0.53571429,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.53205128,  1.        ],\n",
       "       [ 0.98      ,  0.96428571,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.92948718,  0.        ],\n",
       "       [ 0.52      ,  0.53571429,  0.25      ,  0.625     ,  0.57142857,\n",
       "         0.58974359,  1.        ],\n",
       "       [ 0.7       ,  0.64285714,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.69230769,  1.        ],\n",
       "       [ 0.42      ,  0.32142857,  0.25      ,  0.375     ,  0.57142857,\n",
       "         0.49358974,  1.        ],\n",
       "       [ 0.6       ,  0.42857143,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.62179487,  1.        ],\n",
       "       [ 0.74      ,  0.39285714,  0.5       ,  0.75      ,  0.71428571,\n",
       "         0.48076923,  1.        ],\n",
       "       [ 0.62      ,  0.67857143,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.65064103,  1.        ],\n",
       "       [ 0.56      ,  0.5       ,  0.25      ,  0.75      ,  0.71428571,\n",
       "         0.35897436,  1.        ],\n",
       "       [ 0.48      ,  0.53571429,  0.25      ,  0.375     ,  0.71428571,\n",
       "         0.47115385,  0.        ],\n",
       "       [ 0.06      ,  0.17857143,  0.25      ,  0.25      ,  0.71428571,\n",
       "         0.32051282,  1.        ],\n",
       "       [ 0.74      ,  0.42857143,  1.        ,  0.5       ,  0.57142857,\n",
       "         0.65384615,  1.        ],\n",
       "       [ 0.74      ,  0.78571429,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.71153846,  0.        ],\n",
       "       [ 0.62      ,  0.60714286,  0.5       ,  0.625     ,  0.57142857,\n",
       "         0.64102564,  1.        ],\n",
       "       [ 0.68      ,  0.71428571,  1.        ,  1.        ,  1.        ,\n",
       "         0.73076923,  1.        ],\n",
       "       [ 0.32      ,  0.39285714,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.45192308,  0.        ],\n",
       "       [ 0.46      ,  0.60714286,  0.5       ,  0.75      ,  0.57142857,\n",
       "         0.70512821,  0.        ],\n",
       "       [ 0.82      ,  0.82142857,  1.        ,  0.75      ,  0.57142857,\n",
       "         0.84615385,  1.        ],\n",
       "       [ 0.5       ,  0.46428571,  0.5       ,  0.25      ,  0.28571429,\n",
       "         0.53846154,  0.        ],\n",
       "       [ 0.44      ,  0.21428571,  0.        ,  0.625     ,  0.42857143,\n",
       "         0.44230769,  1.        ],\n",
       "       [ 0.42      ,  0.53571429,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.70512821,  1.        ],\n",
       "       [ 0.76      ,  0.57142857,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.76282051,  1.        ],\n",
       "       [ 0.16      ,  0.32142857,  0.75      ,  0.375     ,  0.85714286,\n",
       "         0.28525641,  1.        ],\n",
       "       [ 0.7       ,  0.71428571,  0.75      ,  0.625     ,  0.57142857,\n",
       "         0.67948718,  0.        ],\n",
       "       [ 0.5       ,  0.46428571,  0.25      ,  0.25      ,  0.28571429,\n",
       "         0.2724359 ,  0.        ],\n",
       "       [ 0.82      ,  0.71428571,  1.        ,  0.75      ,  1.        ,\n",
       "         0.96153846,  1.        ],\n",
       "       [ 0.36      ,  0.5       ,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.46153846,  0.        ],\n",
       "       [ 0.66      ,  0.75      ,  0.75      ,  0.75      ,  0.85714286,\n",
       "         0.77884615,  1.        ],\n",
       "       [ 0.5       ,  0.64285714,  0.25      ,  0.625     ,  0.42857143,\n",
       "         0.53205128,  1.        ],\n",
       "       [ 0.34      ,  0.35714286,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.47115385,  0.        ],\n",
       "       [ 0.66      ,  0.64285714,  0.5       ,  0.75      ,  0.57142857,\n",
       "         0.73717949,  1.        ],\n",
       "       [ 0.28      ,  0.28571429,  0.25      ,  0.375     ,  0.57142857,\n",
       "         0.40705128,  0.        ],\n",
       "       [ 0.82      ,  0.85714286,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.84615385,  1.        ],\n",
       "       [ 0.52      ,  0.21428571,  0.        ,  0.125     ,  0.14285714,\n",
       "         0.20192308,  0.        ],\n",
       "       [ 0.72      ,  0.71428571,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.73717949,  1.        ],\n",
       "       [ 0.6       ,  0.32142857,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.58333333,  0.        ],\n",
       "       [ 0.48      ,  0.39285714,  0.25      ,  0.25      ,  0.42857143,\n",
       "         0.45192308,  0.        ],\n",
       "       [ 0.5       ,  0.46428571,  0.25      ,  0.5       ,  0.42857143,\n",
       "         0.49358974,  0.        ],\n",
       "       [ 0.88      ,  0.85714286,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.87820513,  1.        ],\n",
       "       [ 0.18      ,  0.28571429,  0.5       ,  0.25      ,  0.14285714,\n",
       "         0.39102564,  0.        ],\n",
       "       [ 0.42      ,  0.42857143,  0.5       ,  0.75      ,  0.57142857,\n",
       "         0.42628205,  1.        ],\n",
       "       [ 0.56      ,  0.32142857,  1.        ,  0.625     ,  1.        ,\n",
       "         0.63461538,  1.        ],\n",
       "       [ 0.86      ,  0.96428571,  1.        ,  1.        ,  0.85714286,\n",
       "         0.95512821,  1.        ],\n",
       "       [ 0.74      ,  0.75      ,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.59615385,  1.        ],\n",
       "       [ 0.14      ,  0.14285714,  0.25      ,  0.375     ,  0.        ,\n",
       "         0.34935897,  0.        ],\n",
       "       [ 0.54      ,  0.28571429,  0.25      ,  0.5       ,  0.28571429,\n",
       "         0.56730769,  0.        ],\n",
       "       [ 0.5       ,  0.42857143,  0.5       ,  0.75      ,  0.28571429,\n",
       "         0.41666667,  0.        ],\n",
       "       [ 0.64      ,  0.39285714,  0.75      ,  0.5       ,  0.28571429,\n",
       "         0.39102564,  1.        ],\n",
       "       [ 0.48      ,  0.5       ,  0.25      ,  0.75      ,  0.57142857,\n",
       "         0.46474359,  0.        ],\n",
       "       [ 0.78      ,  0.67857143,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.70833333,  1.        ],\n",
       "       [ 0.82      ,  0.89285714,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.83974359,  1.        ],\n",
       "       [ 0.8       ,  0.82142857,  1.        ,  0.875     ,  0.42857143,\n",
       "         0.81410256,  1.        ],\n",
       "       [ 0.36      ,  0.35714286,  0.25      ,  0.25      ,  0.57142857,\n",
       "         0.37820513,  1.        ],\n",
       "       [ 0.5       ,  0.32142857,  0.5       ,  0.625     ,  0.85714286,\n",
       "         0.74679487,  0.        ],\n",
       "       [ 0.28      ,  0.46428571,  0.25      ,  0.5       ,  0.42857143,\n",
       "         0.44871795,  1.        ],\n",
       "       [ 0.44      ,  0.53571429,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.59294872,  1.        ],\n",
       "       [ 0.38      ,  0.5       ,  0.25      ,  0.375     ,  0.28571429,\n",
       "         0.38461538,  0.        ],\n",
       "       [ 0.58      ,  0.5       ,  0.5       ,  0.75      ,  0.42857143,\n",
       "         0.38461538,  1.        ],\n",
       "       [ 0.56      ,  0.53571429,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.47115385,  1.        ],\n",
       "       [ 0.18      ,  0.07142857,  0.        ,  0.        , -0.14285714,\n",
       "         0.17307692,  0.        ],\n",
       "       [ 0.58      ,  0.39285714,  0.75      ,  0.875     ,  0.57142857,\n",
       "         0.59615385,  0.        ],\n",
       "       [ 0.68      ,  0.28571429,  0.5       ,  0.75      ,  1.        ,\n",
       "         0.58974359,  1.        ],\n",
       "       [ 0.8       ,  0.78571429,  0.75      ,  0.875     ,  0.42857143,\n",
       "         0.75961538,  1.        ],\n",
       "       [ 0.9       ,  0.89285714,  1.        ,  1.        ,  1.        ,\n",
       "         0.96794872,  1.        ],\n",
       "       [ 0.36      ,  0.42857143,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.40705128,  0.        ],\n",
       "       [ 0.6       ,  0.57142857,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.52564103,  1.        ],\n",
       "       [ 0.68      ,  0.53571429,  1.        ,  0.625     ,  0.71428571,\n",
       "         0.59615385,  1.        ],\n",
       "       [ 1.        ,  0.71428571,  0.75      ,  1.        ,  0.85714286,\n",
       "         0.91666667,  1.        ],\n",
       "       [ 0.56      ,  0.60714286,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.54487179,  0.        ],\n",
       "       [ 0.3       ,  0.35714286,  0.25      ,  0.25      ,  0.28571429,\n",
       "         0.44230769,  0.        ],\n",
       "       [ 0.5       ,  0.42857143,  0.5       ,  0.5       ,  0.28571429,\n",
       "         0.49038462,  0.        ],\n",
       "       [ 0.72      ,  0.78571429,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.74038462,  1.        ],\n",
       "       [ 0.4       ,  0.25      ,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.16025641,  0.        ],\n",
       "       [ 0.96      ,  0.89285714,  0.75      ,  0.625     ,  0.85714286,\n",
       "         0.8525641 ,  1.        ],\n",
       "       [ 0.28      ,  0.53571429,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.33974359,  0.        ],\n",
       "       [ 0.84      ,  0.57142857,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.71153846,  1.        ],\n",
       "       [ 0.66      ,  0.67857143,  1.        ,  0.75      ,  1.        ,\n",
       "         0.98076923,  1.        ],\n",
       "       [ 0.6       ,  0.42857143,  0.5       ,  0.5       ,  0.28571429,\n",
       "         0.56730769,  1.        ],\n",
       "       [ 0.48      ,  0.60714286,  0.75      ,  0.625     ,  0.71428571,\n",
       "         0.63141026,  1.        ],\n",
       "       [ 0.74      ,  0.57142857,  1.        ,  1.        ,  0.57142857,\n",
       "         0.74679487,  1.        ],\n",
       "       [ 0.        ,  0.42857143,  0.75      ,  0.25      ,  0.28571429,\n",
       "         0.21153846,  0.        ],\n",
       "       [ 0.9       ,  0.92857143,  1.        ,  0.875     ,  0.57142857,\n",
       "         0.84615385,  1.        ],\n",
       "       [ 0.74      ,  0.5       ,  0.75      ,  0.75      ,  0.85714286,\n",
       "         0.625     ,  1.        ],\n",
       "       [ 0.64      ,  0.64285714,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.69551282,  0.        ],\n",
       "       [ 0.36      ,  0.60714286,  0.25      ,  0.5       ,  0.71428571,\n",
       "         0.52884615,  0.        ],\n",
       "       [ 0.68      ,  0.75      ,  1.        ,  0.75      ,  1.        ,\n",
       "         0.78525641,  1.        ],\n",
       "       [ 0.82      ,  0.85714286,  1.        ,  0.75      ,  0.71428571,\n",
       "         0.78846154,  1.        ],\n",
       "       [ 0.14      ,  0.25      ,  0.75      ,  0.5       ,  0.57142857,\n",
       "         0.32371795,  0.        ],\n",
       "       [ 0.28      ,  0.42857143,  0.5       ,  0.375     ,  0.14285714,\n",
       "         0.42307692,  0.        ],\n",
       "       [ 0.38      ,  0.46428571,  0.75      ,  0.625     ,  0.14285714,\n",
       "         0.44230769,  0.        ],\n",
       "       [ 0.62      ,  0.35714286,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.70833333,  1.        ],\n",
       "       [ 0.52      ,  0.39285714,  0.5       ,  0.625     ,  0.14285714,\n",
       "         0.28205128,  0.        ],\n",
       "       [ 0.62      ,  0.60714286,  0.5       ,  0.5       ,  0.71428571,\n",
       "         0.44871795,  1.        ],\n",
       "       [ 0.64      ,  0.71428571,  0.75      ,  0.625     ,  0.28571429,\n",
       "         0.71153846,  1.        ],\n",
       "       [ 0.74      ,  0.67857143,  0.75      ,  0.75      ,  0.85714286,\n",
       "         0.70512821,  1.        ],\n",
       "       [ 0.64      ,  0.78571429,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.68589744,  1.        ],\n",
       "       [ 0.16      ,  0.21428571,  0.25      ,  0.75      ,  0.42857143,\n",
       "         0.39423077,  0.        ],\n",
       "       [ 0.52      ,  0.64285714,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.56410256,  0.        ],\n",
       "       [ 0.3       ,  0.57142857,  1.        ,  0.5       ,  0.42857143,\n",
       "         0.53846154,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(7,activation='relu',input_dim=7))\n",
    "model.add(Dense(7,activation='relu'))\n",
    "model.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 56        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120 (480.00 Byte)\n",
      "Trainable params: 120 (480.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 1s 25ms/step - loss: 0.2114 - val_loss: 0.1867\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1493 - val_loss: 0.1263\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0997 - val_loss: 0.0829\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0652 - val_loss: 0.0552\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0446 - val_loss: 0.0411\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0343 - val_loss: 0.0356\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0309 - val_loss: 0.0338\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0294 - val_loss: 0.0328\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0286 - val_loss: 0.0318\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 0.0307\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0269 - val_loss: 0.0297\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0262 - val_loss: 0.0287\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0254 - val_loss: 0.0277\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0247 - val_loss: 0.0269\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0240 - val_loss: 0.0261\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0233 - val_loss: 0.0253\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0227 - val_loss: 0.0245\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0221 - val_loss: 0.0238\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0215 - val_loss: 0.0231\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0209 - val_loss: 0.0225\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0203 - val_loss: 0.0218\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0198 - val_loss: 0.0212\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0193 - val_loss: 0.0206\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0188 - val_loss: 0.0201\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0183 - val_loss: 0.0195\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0178 - val_loss: 0.0189\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0174 - val_loss: 0.0184\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0169 - val_loss: 0.0179\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0165 - val_loss: 0.0174\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 0.0169\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0157 - val_loss: 0.0164\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0153 - val_loss: 0.0160\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0149 - val_loss: 0.0155\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0145 - val_loss: 0.0151\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0142 - val_loss: 0.0147\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0143\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0139\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0131 - val_loss: 0.0135\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0128 - val_loss: 0.0131\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0127\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0122 - val_loss: 0.0124\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0119 - val_loss: 0.0120\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0117\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0111\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0105\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0094 - val_loss: 0.0091\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0092 - val_loss: 0.0088\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 0.0086\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0088 - val_loss: 0.0084\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0081\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0082 - val_loss: 0.0077\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0073\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0075 - val_loss: 0.0069\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0067\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0062\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.0066 - val_loss: 0.0058\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0062 - val_loss: 0.0052\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0050\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0047\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0045\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0044\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0055 - val_loss: 0.0042\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0043\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0042\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0041\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0042\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0051 - val_loss: 0.0041\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.0050 - val_loss: 0.0040\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0040\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s 16ms/step - loss: 0.0048 - val_loss: 0.0039\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0039\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.0047 - val_loss: 0.0039\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s 15ms/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 0.0039\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled,y_train,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685349094590397"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21391778090>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
